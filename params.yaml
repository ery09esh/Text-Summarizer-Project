# TrainingArguments:
#   num_train_epochs: 1
#   warmup_steps: 500
#   per_device_train_batch_size: 1
#   weight_decay: 0.01
#   logging_steps: 10
#   evaluation_strategy: steps
#   eval_steps: 500
#   save_steps: 1e6
#   gradient_accumulation_steps: 16


TrainingArguments:
  num_train_epochs: 1
  warmup_steps: 500
  per_device_train_batch_size: 1  # Keep low due to 4GB GPU memory
  weight_decay: 0.01
  logging_steps: 10
  evaluation_strategy: steps
  eval_steps: 1000  # Increase eval steps to reduce evaluation frequency
  save_steps: 1e6
  gradient_accumulation_steps: 32  # Increased for larger effective batch size
  fp16: true  # Enable mixed precision to save GPU memory
